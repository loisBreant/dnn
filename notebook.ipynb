{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73d0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 19 11:11:52 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "085870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9be9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ok!\n"
     ]
    }
   ],
   "source": [
    "# ok\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gpu_id = \"0\" \n",
    "    lowlight_images_path = \"data/train_data/\"\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.0001\n",
    "    grad_clip_norm = 0.1\n",
    "    num_epochs = 200\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 4\n",
    "    num_workers = 4\n",
    "    display_iter = 10\n",
    "    snapshot_iter = 10\n",
    "    snapshots_folder = \"snapshots/\"\n",
    "    train_images_path = \"data/train/\" \n",
    "    load_pretrain = False\n",
    "    pretrain_dir = \"snapshots/model_epoch_100.pth\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = Config.gpu_id\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU ok!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"You must use GPU!\")\n",
    "\n",
    "if not os.path.exists(Config.snapshots_folder):\n",
    "    os.makedirs(Config.snapshots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21a600a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3\n",
      "From (redirected): https://drive.google.com/uc?id=1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3&confirm=t&uuid=53e94f0e-1b87-478e-9784-06819fe5d48e\n",
      "To: /content/dataset.zip\n",
      "100%|██████████| 72.9M/72.9M [00:01<00:00, 43.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images trouvées : 2002\n"
     ]
    }
   ],
   "source": [
    "file_id = '1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output_zip = 'dataset.zip'\n",
    "destination_folder = 'data/train_data/'\n",
    "\n",
    "gdown.download(url, output_zip, quiet=False)\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "!unzip -q -j {output_zip} -d {destination_folder}\n",
    "\n",
    "os.remove(output_zip)\n",
    "print(f\"Nombre d'images trouvées : {len(os.listdir(destination_folder))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d11c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok !\n",
    "\n",
    "class L_color(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_color, self).__init__()\n",
    "\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Drg = torch.pow(mr-mg,2)\n",
    "        Drb = torch.pow(mr-mb,2)\n",
    "        Dgb = torch.pow(mb-mg,2)\n",
    "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
    "\n",
    "\n",
    "        return k\n",
    "\n",
    "\t\t\t\n",
    "class L_spa(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_spa, self).__init__()\n",
    "        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
    "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
    "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
    "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
    "        self.pool = nn.AvgPool2d(4)\n",
    "    def forward(self, org , enhance ):\n",
    "        b,c,h,w = org.shape\n",
    "\n",
    "        org_mean = torch.mean(org,1,keepdim=True)\n",
    "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
    "\n",
    "        org_pool =  self.pool(org_mean)\t\t\t\n",
    "        enhance_pool = self.pool(enhance_mean)\t\n",
    "\n",
    "        weight_diff =torch.max(torch.FloatTensor([1]).cuda() + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).cuda(),torch.FloatTensor([0]).cuda()),torch.FloatTensor([0.5]).cuda())\n",
    "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).cuda()) ,enhance_pool-org_pool)\n",
    "\n",
    "\n",
    "        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n",
    "        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n",
    "        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n",
    "        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n",
    "        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n",
    "        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n",
    "        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
    "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
    "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
    "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
    "        E = (D_left + D_right + D_up +D_down)\n",
    "        # E = 25*(D_left + D_right + D_up +D_down)\n",
    "\n",
    "        return E\n",
    "class L_exp(nn.Module):\n",
    "\n",
    "    def __init__(self,patch_size,mean_val):\n",
    "        super(L_exp, self).__init__()\n",
    "        # print(1)\n",
    "        self.pool = nn.AvgPool2d(patch_size)\n",
    "        self.mean_val = mean_val\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "        x = torch.mean(x,1,keepdim=True)\n",
    "        mean = self.pool(x)\n",
    "\n",
    "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).cuda(),2))\n",
    "        return d\n",
    "\n",
    "class L_tv(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=1):\n",
    "        super(L_tv,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
    "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "class Sa_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sa_Loss, self).__init__()\n",
    "        # print(1)\n",
    "    def forward(self, x ):\n",
    "        # self.grad = np.ones(x.shape,dtype=np.float32)\n",
    "        b,c,h,w = x.shape\n",
    "        # x_de = x.cpu().detach().numpy()\n",
    "        r,g,b = torch.split(x , 1, dim=1)\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Dr = r-mr\n",
    "        Dg = g-mg\n",
    "        Db = b-mb\n",
    "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
    "        # print(k)\n",
    "        \n",
    "\n",
    "        k = torch.mean(k)\n",
    "        return k\n",
    "\n",
    "class perception_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(perception_loss, self).__init__()\n",
    "        features = vgg16(pretrained=True).features\n",
    "        self.to_relu_1_2 = nn.Sequential() \n",
    "        self.to_relu_2_2 = nn.Sequential() \n",
    "        self.to_relu_3_3 = nn.Sequential()\n",
    "        self.to_relu_4_3 = nn.Sequential()\n",
    "\n",
    "        for x in range(4):\n",
    "            self.to_relu_1_2.add_module(str(x), features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.to_relu_2_2.add_module(str(x), features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.to_relu_3_3.add_module(str(x), features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.to_relu_4_3.add_module(str(x), features[x])\n",
    "        \n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.to_relu_1_2(x)\n",
    "        h_relu_1_2 = h\n",
    "        h = self.to_relu_2_2(h)\n",
    "        h_relu_2_2 = h\n",
    "        h = self.to_relu_3_3(h)\n",
    "        h_relu_3_3 = h\n",
    "        h = self.to_relu_4_3(h)\n",
    "        h_relu_4_3 = h\n",
    "        # out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
    "        return h_relu_4_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bdc003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok\n",
    "class enhance_net_nopool(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(enhance_net_nopool, self).__init__()\n",
    "\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\t\tnumber_f = 32\n",
    "\t\tself.e_conv1 = nn.Conv2d(3,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv2 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv3 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv4 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv5 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv6 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv7 = nn.Conv2d(number_f*2,24,3,1,1,bias=True) \n",
    "\n",
    "\t\tself.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n",
    "\t\tself.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx1 = self.relu(self.e_conv1(x))\n",
    "\t\t# p1 = self.maxpool(x1)\n",
    "\t\tx2 = self.relu(self.e_conv2(x1))\n",
    "\t\t# p2 = self.maxpool(x2)\n",
    "\t\tx3 = self.relu(self.e_conv3(x2))\n",
    "\t\t# p3 = self.maxpool(x3)\n",
    "\t\tx4 = self.relu(self.e_conv4(x3))\n",
    "\n",
    "\t\tx5 = self.relu(self.e_conv5(torch.cat([x3,x4],1)))\n",
    "\t\t# x5 = self.upsample(x5)\n",
    "\t\tx6 = self.relu(self.e_conv6(torch.cat([x2,x5],1)))\n",
    "\n",
    "\t\tx_r = F.tanh(self.e_conv7(torch.cat([x1,x6],1)))\n",
    "\t\tr1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, 3, dim=1)\n",
    "\n",
    "\n",
    "\t\tx = x + r1*(torch.pow(x,2)-x)\n",
    "\t\tx = x + r2*(torch.pow(x,2)-x)\n",
    "\t\tx = x + r3*(torch.pow(x,2)-x)\n",
    "\t\tenhance_image_1 = x + r4*(torch.pow(x,2)-x)\t\t\n",
    "\t\tx = enhance_image_1 + r5*(torch.pow(enhance_image_1,2)-enhance_image_1)\t\t\n",
    "\t\tx = x + r6*(torch.pow(x,2)-x)\t\n",
    "\t\tx = x + r7*(torch.pow(x,2)-x)\n",
    "\t\tenhance_image = x + r8*(torch.pow(x,2)-x)\n",
    "\t\tr = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],1)\n",
    "\t\treturn enhance_image_1,enhance_image,r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3dd82960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLightDataset(data.Dataset):\n",
    "    def __init__(self, images_path):\n",
    "        self.train_list = glob.glob(images_path + \"*.jpg\") + glob.glob(images_path + \"*.png\")\n",
    "        self.size = 256\n",
    "        if len(self.train_list) == 0:\n",
    "            print(f\"ERREUR: Aucune image trouvée dans {images_path}\")\n",
    "        else:\n",
    "            print(f\"Dataset chargé : {len(self.train_list)} images.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_lowlight_path = self.train_list[index]\n",
    "        data_lowlight = Image.open(data_lowlight_path).convert('RGB') # Force RGB\n",
    "        \n",
    "        # Redimensionnement sécurisé\n",
    "        try:\n",
    "            resample = Image.Resampling.LANCZOS\n",
    "        except AttributeError:\n",
    "            resample = Image.LANCZOS\n",
    "            \n",
    "        data_lowlight = data_lowlight.resize((self.size, self.size), resample)\n",
    "        \n",
    "        # Normalisation\n",
    "        data_lowlight = (np.asarray(data_lowlight) / 255.0) \n",
    "        data_lowlight = torch.from_numpy(data_lowlight).float()\n",
    "        data_lowlight = data_lowlight.permute(2, 0, 1)\n",
    "\n",
    "        return data_lowlight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c442c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK !\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "random.seed(1143)\n",
    "\n",
    "def populate_train_list(lowlight_images_path):\n",
    "\timage_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n",
    "\ttrain_list = image_list_lowlight\n",
    "\trandom.shuffle(train_list)\n",
    "\treturn train_list\n",
    "\n",
    "\n",
    "class lowlight_loader(data.Dataset):\n",
    "\tdef __init__(self, lowlight_images_path):\n",
    "\t\tself.train_list = populate_train_list(lowlight_images_path) \n",
    "\t\tself.size = 256\n",
    "\n",
    "\t\tself.data_list = self.train_list\n",
    "\t\tprint(\"Total training examples:\", len(self.train_list))\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tdata_lowlight_path = self.data_list[index]\n",
    "\t\tdata_lowlight = Image.open(data_lowlight_path)\n",
    " \n",
    "\t\tresample_method = Image.Resampling.LANCZOS\n",
    "  \n",
    "\t\tdata_lowlight = data_lowlight.resize((self.size,self.size), resample_method)\n",
    "\t\tdata_lowlight = (np.asarray(data_lowlight)/255.0) \n",
    "\t\tdata_lowlight = torch.from_numpy(data_lowlight).float()\n",
    "\n",
    "\t\treturn data_lowlight.permute(2,0,1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b34fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK !\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a5a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/tmp/ipython-input-2689004671.py:44: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(DCE_net.parameters(),config.grad_clip_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 10 : 1.1135104894638062\n",
      "Loss at iteration 20 : 1.4680814743041992\n",
      "Loss at iteration 30 : 1.178105115890503\n",
      "Loss at iteration 40 : 1.6007816791534424\n",
      "Loss at iteration 50 : 1.4286653995513916\n",
      "Loss at iteration 60 : 1.1238524913787842\n",
      "Loss at iteration 70 : 1.4800519943237305\n",
      "Loss at iteration 80 : 1.144879937171936\n",
      "Loss at iteration 90 : 0.8282774686813354\n",
      "Loss at iteration 100 : 1.1178741455078125\n",
      "Loss at iteration 110 : 1.2724099159240723\n",
      "Loss at iteration 120 : 1.1989870071411133\n",
      "Loss at iteration 130 : 1.1099302768707275\n",
      "Loss at iteration 140 : 0.8265196084976196\n",
      "Loss at iteration 150 : 0.8702436089515686\n",
      "Loss at iteration 160 : 1.4869437217712402\n",
      "Loss at iteration 170 : 0.837498664855957\n",
      "Loss at iteration 180 : 1.0604890584945679\n",
      "Loss at iteration 190 : 0.7767890095710754\n",
      "Loss at iteration 200 : 0.8553151488304138\n",
      "Loss at iteration 210 : 0.9618712067604065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2689004671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDCE_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshots_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Epoch\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-2689004671.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_lowlight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mimg_lowlight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_lowlight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0menhanced_image_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menhanced_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mDCE_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_lowlight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def train(config):\n",
    "\n",
    "\tos.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "\tDCE_net = enhance_net_nopool().cuda()\n",
    "\n",
    "\tDCE_net.apply(weights_init)\n",
    "\tif config.load_pretrain == True:\n",
    "\t    DCE_net.load_state_dict(torch.load(config.pretrain_dir))\n",
    "\ttrain_dataset = lowlight_loader(config.lowlight_images_path)\t\t\n",
    "\t\n",
    "\ttrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "\n",
    "\tl_color = L_color()\n",
    "\tl_spa = L_spa()\n",
    "\n",
    "\tl_exp = L_exp(16,0.6)\n",
    "\tl_tv = L_tv()\n",
    " \n",
    "\toptimizer = torch.optim.Adam(DCE_net.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\t\n",
    "\tDCE_net.train()\n",
    "\n",
    "\tfor epoch in range(config.num_epochs):\n",
    "\t\tfor iteration, img_lowlight in enumerate(train_loader):\n",
    "\n",
    "\t\t\timg_lowlight = img_lowlight.cuda()\n",
    "\n",
    "\t\t\tenhanced_image_1,enhanced_image,A  = DCE_net(img_lowlight)\n",
    "\n",
    "\t\t\tLoss_TV = 200*l_tv(A)\n",
    "\t\t\tloss_spa = torch.mean(l_spa(enhanced_image, img_lowlight))\n",
    "\t\t\tloss_col = 5*torch.mean(l_color(enhanced_image))\n",
    "\t\t\tloss_exp = 10*torch.mean(l_exp(enhanced_image))\t\n",
    "\t\t\t\n",
    "\t\t\t# best_loss\n",
    "\t\t\tloss =  Loss_TV + loss_spa + loss_col + loss_exp\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm(DCE_net.parameters(),config.grad_clip_norm)\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tif ((iteration+1) % config.display_iter) == 0:\n",
    "\t\t\t\tprint(\"Loss at iteration\", iteration+1, \":\", loss.item())\n",
    "\t\t\tif ((iteration+1) % config.snapshot_iter) == 0:\n",
    "\t\t\t\t\n",
    "\t\t\t\ttorch.save(DCE_net.state_dict(), config.snapshots_folder + \"Epoch\" + str(epoch) + '.pth') \t\t\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4f5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
