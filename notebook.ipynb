{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 19 11:38:24 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P0             29W /   70W |    2052MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "085870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import gdown\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db9be9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ok!\n"
     ]
    }
   ],
   "source": [
    "# ok\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gpu_id = \"0\" \n",
    "    lowlight_images_path = \"data/train_data/\"\n",
    "    lr = 0.0001\n",
    "    weight_decay = 0.0001\n",
    "    grad_clip_norm = 0.1\n",
    "    num_epochs = 2\n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 4\n",
    "    num_workers = 4\n",
    "    display_iter = 10\n",
    "    snapshot_iter = 10\n",
    "    snapshots_folder = \"snapshots/\"\n",
    "    train_images_path = \"data/train/\" \n",
    "    load_pretrain = False\n",
    "    pretrain_dir = \"snapshots/model_epoch_100.pth\"\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = Config.gpu_id\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU ok!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"You must use GPU!\")\n",
    "\n",
    "if not os.path.exists(Config.snapshots_folder):\n",
    "    os.makedirs(Config.snapshots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21a600a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3\n",
      "From (redirected): https://drive.google.com/uc?id=1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3&confirm=t&uuid=f8f69774-82c7-48f4-9c2c-9b840c2297d4\n",
      "To: /content/dataset.zip\n",
      "100%|██████████| 72.9M/72.9M [00:00<00:00, 157MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images trouvées : 2002\n"
     ]
    }
   ],
   "source": [
    "file_id = '1GAB3uGsmAyLgtDBDONbil08vVu5wJcG3'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output_zip = 'dataset.zip'\n",
    "destination_folder = 'data/train_data/'\n",
    "\n",
    "gdown.download(url, output_zip, quiet=False)\n",
    "\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "!rm -rf {destination_folder}/*\n",
    "!unzip -q -j {output_zip} -d {destination_folder}\n",
    "\n",
    "os.remove(output_zip)\n",
    "print(f\"Nombre d'images trouvées : {len(os.listdir(destination_folder))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d11c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok !\n",
    "\n",
    "class L_color(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_color, self).__init__()\n",
    "\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Drg = torch.pow(mr-mg,2)\n",
    "        Drb = torch.pow(mr-mb,2)\n",
    "        Dgb = torch.pow(mb-mg,2)\n",
    "        k = torch.pow(torch.pow(Drg,2) + torch.pow(Drb,2) + torch.pow(Dgb,2),0.5)\n",
    "\n",
    "\n",
    "        return k\n",
    "\n",
    "\t\t\t\n",
    "class L_spa(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(L_spa, self).__init__()\n",
    "        # print(1)kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)\n",
    "        kernel_left = torch.FloatTensor( [[0,0,0],[-1,1,0],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_right = torch.FloatTensor( [[0,0,0],[0,1,-1],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_up = torch.FloatTensor( [[0,-1,0],[0,1, 0 ],[0,0,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        kernel_down = torch.FloatTensor( [[0,0,0],[0,1, 0],[0,-1,0]]).cuda().unsqueeze(0).unsqueeze(0)\n",
    "        self.weight_left = nn.Parameter(data=kernel_left, requires_grad=False)\n",
    "        self.weight_right = nn.Parameter(data=kernel_right, requires_grad=False)\n",
    "        self.weight_up = nn.Parameter(data=kernel_up, requires_grad=False)\n",
    "        self.weight_down = nn.Parameter(data=kernel_down, requires_grad=False)\n",
    "        self.pool = nn.AvgPool2d(4)\n",
    "    def forward(self, org , enhance ):\n",
    "        b,c,h,w = org.shape\n",
    "\n",
    "        org_mean = torch.mean(org,1,keepdim=True)\n",
    "        enhance_mean = torch.mean(enhance,1,keepdim=True)\n",
    "\n",
    "        org_pool =  self.pool(org_mean)\t\t\t\n",
    "        enhance_pool = self.pool(enhance_mean)\t\n",
    "\n",
    "        weight_diff =torch.max(torch.FloatTensor([1]).cuda() + 10000*torch.min(org_pool - torch.FloatTensor([0.3]).cuda(),torch.FloatTensor([0]).cuda()),torch.FloatTensor([0.5]).cuda())\n",
    "        E_1 = torch.mul(torch.sign(enhance_pool - torch.FloatTensor([0.5]).cuda()) ,enhance_pool-org_pool)\n",
    "\n",
    "\n",
    "        D_org_letf = F.conv2d(org_pool , self.weight_left, padding=1)\n",
    "        D_org_right = F.conv2d(org_pool , self.weight_right, padding=1)\n",
    "        D_org_up = F.conv2d(org_pool , self.weight_up, padding=1)\n",
    "        D_org_down = F.conv2d(org_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_enhance_letf = F.conv2d(enhance_pool , self.weight_left, padding=1)\n",
    "        D_enhance_right = F.conv2d(enhance_pool , self.weight_right, padding=1)\n",
    "        D_enhance_up = F.conv2d(enhance_pool , self.weight_up, padding=1)\n",
    "        D_enhance_down = F.conv2d(enhance_pool , self.weight_down, padding=1)\n",
    "\n",
    "        D_left = torch.pow(D_org_letf - D_enhance_letf,2)\n",
    "        D_right = torch.pow(D_org_right - D_enhance_right,2)\n",
    "        D_up = torch.pow(D_org_up - D_enhance_up,2)\n",
    "        D_down = torch.pow(D_org_down - D_enhance_down,2)\n",
    "        E = (D_left + D_right + D_up +D_down)\n",
    "        # E = 25*(D_left + D_right + D_up +D_down)\n",
    "\n",
    "        return E\n",
    "class L_exp(nn.Module):\n",
    "\n",
    "    def __init__(self,patch_size,mean_val):\n",
    "        super(L_exp, self).__init__()\n",
    "        # print(1)\n",
    "        self.pool = nn.AvgPool2d(patch_size)\n",
    "        self.mean_val = mean_val\n",
    "    def forward(self, x ):\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "        x = torch.mean(x,1,keepdim=True)\n",
    "        mean = self.pool(x)\n",
    "\n",
    "        d = torch.mean(torch.pow(mean- torch.FloatTensor([self.mean_val] ).cuda(),2))\n",
    "        return d\n",
    "\n",
    "class L_tv(nn.Module):\n",
    "    def __init__(self,TVLoss_weight=1):\n",
    "        super(L_tv,self).__init__()\n",
    "        self.TVLoss_weight = TVLoss_weight\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h =  (x.size()[2]-1) * x.size()[3]\n",
    "        count_w = x.size()[2] * (x.size()[3] - 1)\n",
    "        h_tv = torch.pow((x[:,:,1:,:]-x[:,:,:h_x-1,:]),2).sum()\n",
    "        w_tv = torch.pow((x[:,:,:,1:]-x[:,:,:,:w_x-1]),2).sum()\n",
    "        return self.TVLoss_weight*2*(h_tv/count_h+w_tv/count_w)/batch_size\n",
    "class Sa_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sa_Loss, self).__init__()\n",
    "        # print(1)\n",
    "    def forward(self, x ):\n",
    "        # self.grad = np.ones(x.shape,dtype=np.float32)\n",
    "        b,c,h,w = x.shape\n",
    "        # x_de = x.cpu().detach().numpy()\n",
    "        r,g,b = torch.split(x , 1, dim=1)\n",
    "        mean_rgb = torch.mean(x,[2,3],keepdim=True)\n",
    "        mr,mg, mb = torch.split(mean_rgb, 1, dim=1)\n",
    "        Dr = r-mr\n",
    "        Dg = g-mg\n",
    "        Db = b-mb\n",
    "        k =torch.pow( torch.pow(Dr,2) + torch.pow(Db,2) + torch.pow(Dg,2),0.5)\n",
    "        # print(k)\n",
    "        \n",
    "\n",
    "        k = torch.mean(k)\n",
    "        return k\n",
    "\n",
    "class perception_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(perception_loss, self).__init__()\n",
    "        features = vgg16(pretrained=True).features\n",
    "        self.to_relu_1_2 = nn.Sequential() \n",
    "        self.to_relu_2_2 = nn.Sequential() \n",
    "        self.to_relu_3_3 = nn.Sequential()\n",
    "        self.to_relu_4_3 = nn.Sequential()\n",
    "\n",
    "        for x in range(4):\n",
    "            self.to_relu_1_2.add_module(str(x), features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.to_relu_2_2.add_module(str(x), features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.to_relu_3_3.add_module(str(x), features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.to_relu_4_3.add_module(str(x), features[x])\n",
    "        \n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.to_relu_1_2(x)\n",
    "        h_relu_1_2 = h\n",
    "        h = self.to_relu_2_2(h)\n",
    "        h_relu_2_2 = h\n",
    "        h = self.to_relu_3_3(h)\n",
    "        h_relu_3_3 = h\n",
    "        h = self.to_relu_4_3(h)\n",
    "        h_relu_4_3 = h\n",
    "        # out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
    "        return h_relu_4_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bdc003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok\n",
    "class enhance_net_nopool(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(enhance_net_nopool, self).__init__()\n",
    "\n",
    "\t\tself.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "\t\tnumber_f = 32\n",
    "\t\tself.e_conv1 = nn.Conv2d(3,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv2 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv3 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv4 = nn.Conv2d(number_f,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv5 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv6 = nn.Conv2d(number_f*2,number_f,3,1,1,bias=True) \n",
    "\t\tself.e_conv7 = nn.Conv2d(number_f*2,24,3,1,1,bias=True) \n",
    "\n",
    "\t\tself.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n",
    "\t\tself.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "\n",
    "\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx1 = self.relu(self.e_conv1(x))\n",
    "\t\t# p1 = self.maxpool(x1)\n",
    "\t\tx2 = self.relu(self.e_conv2(x1))\n",
    "\t\t# p2 = self.maxpool(x2)\n",
    "\t\tx3 = self.relu(self.e_conv3(x2))\n",
    "\t\t# p3 = self.maxpool(x3)\n",
    "\t\tx4 = self.relu(self.e_conv4(x3))\n",
    "\n",
    "\t\tx5 = self.relu(self.e_conv5(torch.cat([x3,x4],1)))\n",
    "\t\t# x5 = self.upsample(x5)\n",
    "\t\tx6 = self.relu(self.e_conv6(torch.cat([x2,x5],1)))\n",
    "\n",
    "\t\tx_r = F.tanh(self.e_conv7(torch.cat([x1,x6],1)))\n",
    "\t\tr1,r2,r3,r4,r5,r6,r7,r8 = torch.split(x_r, 3, dim=1)\n",
    "\n",
    "\n",
    "\t\tx = x + r1*(torch.pow(x,2)-x)\n",
    "\t\tx = x + r2*(torch.pow(x,2)-x)\n",
    "\t\tx = x + r3*(torch.pow(x,2)-x)\n",
    "\t\tenhance_image_1 = x + r4*(torch.pow(x,2)-x)\t\t\n",
    "\t\tx = enhance_image_1 + r5*(torch.pow(enhance_image_1,2)-enhance_image_1)\t\t\n",
    "\t\tx = x + r6*(torch.pow(x,2)-x)\t\n",
    "\t\tx = x + r7*(torch.pow(x,2)-x)\n",
    "\t\tenhance_image = x + r8*(torch.pow(x,2)-x)\n",
    "\t\tr = torch.cat([r1,r2,r3,r4,r5,r6,r7,r8],1)\n",
    "\t\treturn enhance_image_1,enhance_image,r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd82960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLightDataset(data.Dataset):\n",
    "    def __init__(self, images_path):\n",
    "        self.train_list = glob.glob(images_path + \"*.jpg\") + glob.glob(images_path + \"*.png\")\n",
    "        self.size = 256\n",
    "        if len(self.train_list) == 0:\n",
    "            print(f\"ERREUR: Aucune image trouvée dans {images_path}\")\n",
    "        else:\n",
    "            print(f\"Dataset chargé : {len(self.train_list)} images.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_lowlight_path = self.train_list[index]\n",
    "        data_lowlight = Image.open(data_lowlight_path).convert('RGB')\n",
    "        \n",
    "        try:\n",
    "            resample = Image.Resampling.LANCZOS\n",
    "        except AttributeError:\n",
    "            resample = Image.LANCZOS\n",
    "\n",
    "        data_lowlight = data_lowlight.resize((self.size, self.size), resample)\n",
    "        \n",
    "        # Normalisation\n",
    "        data_lowlight = (np.asarray(data_lowlight) / 255.0) \n",
    "        data_lowlight = torch.from_numpy(data_lowlight).float()\n",
    "        data_lowlight = data_lowlight.permute(2, 0, 1)\n",
    "\n",
    "        return data_lowlight\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90c442c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK !\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "random.seed(1143)\n",
    "\n",
    "def populate_train_list(lowlight_images_path):\n",
    "\timage_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n",
    "\ttrain_list = image_list_lowlight\n",
    "\trandom.shuffle(train_list)\n",
    "\treturn train_list\n",
    "\n",
    "\n",
    "class lowlight_loader(data.Dataset):\n",
    "\tdef __init__(self, lowlight_images_path):\n",
    "\t\tself.train_list = populate_train_list(lowlight_images_path) \n",
    "\t\tself.size = 256\n",
    "\n",
    "\t\tself.data_list = self.train_list\n",
    "\t\tprint(\"Total training examples:\", len(self.train_list))\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tdata_lowlight_path = self.data_list[index]\n",
    "\t\tdata_lowlight = Image.open(data_lowlight_path)\n",
    " \n",
    "\t\tresample_method = Image.Resampling.LANCZOS\n",
    "  \n",
    "\t\tdata_lowlight = data_lowlight.resize((self.size,self.size), resample_method)\n",
    "\t\tdata_lowlight = (np.asarray(data_lowlight)/255.0) \n",
    "\t\tdata_lowlight = torch.from_numpy(data_lowlight).float()\n",
    "\n",
    "\t\treturn data_lowlight.permute(2,0,1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87b34fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK !\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1814b56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3742588838.py:45: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(DCE_net.parameters(),config.grad_clip_norm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/2] Iter 10 Loss: 1.1598\n",
      "Epoch [0/2] Iter 20 Loss: 1.0013\n",
      "Epoch [0/2] Iter 30 Loss: 1.0591\n",
      "Epoch [0/2] Iter 40 Loss: 1.4621\n",
      "Epoch [0/2] Iter 50 Loss: 1.0413\n",
      "Epoch [0/2] Iter 60 Loss: 1.2663\n",
      "Epoch [0/2] Iter 70 Loss: 0.9155\n",
      "Epoch [0/2] Iter 80 Loss: 1.3820\n",
      "Epoch [0/2] Iter 90 Loss: 1.2349\n",
      "Epoch [0/2] Iter 100 Loss: 1.4907\n",
      "Epoch [0/2] Iter 110 Loss: 0.8490\n",
      "Epoch [0/2] Iter 120 Loss: 1.1270\n",
      "Epoch [0/2] Iter 130 Loss: 1.0462\n",
      "Epoch [0/2] Iter 140 Loss: 0.6735\n",
      "Epoch [0/2] Iter 150 Loss: 0.9886\n",
      "Epoch [0/2] Iter 160 Loss: 0.7290\n",
      "Epoch [0/2] Iter 170 Loss: 1.3045\n",
      "Epoch [0/2] Iter 180 Loss: 1.1083\n",
      "Epoch [0/2] Iter 190 Loss: 1.3040\n",
      "Epoch [0/2] Iter 200 Loss: 0.7286\n",
      "Epoch [0/2] Iter 210 Loss: 0.8176\n",
      "Epoch [0/2] Iter 220 Loss: 0.6423\n",
      "Epoch [0/2] Iter 230 Loss: 0.7265\n",
      "Epoch [0/2] Iter 240 Loss: 1.0855\n",
      "Epoch [0/2] Iter 250 Loss: 0.8552\n",
      "Epoch [1/2] Iter 10 Loss: 0.8114\n",
      "Epoch [1/2] Iter 20 Loss: 1.0658\n",
      "Epoch [1/2] Iter 30 Loss: 0.9363\n",
      "Epoch [1/2] Iter 40 Loss: 0.8449\n",
      "Epoch [1/2] Iter 50 Loss: 0.7158\n",
      "Epoch [1/2] Iter 60 Loss: 1.2160\n",
      "Epoch [1/2] Iter 70 Loss: 0.8549\n",
      "Epoch [1/2] Iter 80 Loss: 1.0290\n",
      "Epoch [1/2] Iter 90 Loss: 0.9092\n",
      "Epoch [1/2] Iter 100 Loss: 0.9406\n",
      "Epoch [1/2] Iter 110 Loss: 0.8347\n",
      "Epoch [1/2] Iter 120 Loss: 1.1099\n",
      "Epoch [1/2] Iter 130 Loss: 0.7721\n",
      "Epoch [1/2] Iter 140 Loss: 0.9293\n",
      "Epoch [1/2] Iter 150 Loss: 0.8672\n",
      "Epoch [1/2] Iter 160 Loss: 0.6657\n",
      "Epoch [1/2] Iter 170 Loss: 1.0073\n",
      "Epoch [1/2] Iter 180 Loss: 0.9430\n",
      "Epoch [1/2] Iter 190 Loss: 0.7849\n",
      "Epoch [1/2] Iter 200 Loss: 0.8886\n",
      "Epoch [1/2] Iter 210 Loss: 0.9348\n",
      "Epoch [1/2] Iter 220 Loss: 1.2781\n",
      "Epoch [1/2] Iter 230 Loss: 0.9879\n",
      "Epoch [1/2] Iter 240 Loss: 0.9202\n",
      "Epoch [1/2] Iter 250 Loss: 0.9179\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from csv import writer\n",
    "\n",
    "\n",
    "def train(config):\n",
    "\tos.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\twriter = SummaryWriter('logs/exp_1')\n",
    "\n",
    "\tDCE_net = enhance_net_nopool().cuda()\n",
    "\n",
    "\tDCE_net.apply(weights_init)\n",
    "\tif config.load_pretrain == True:\n",
    "\t    DCE_net.load_state_dict(torch.load(config.pretrain_dir))\n",
    "\ttrain_dataset = lowlight_loader(config.lowlight_images_path)\t\t\n",
    "\t\n",
    "\ttrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.train_batch_size, shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "\n",
    "\tl_color = L_color()\n",
    "\tl_spa = L_spa()\n",
    "\n",
    "\tl_exp = L_exp(16,0.6)\n",
    "\tl_tv = L_tv()\n",
    " \n",
    "\toptimizer = torch.optim.Adam(DCE_net.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\t\n",
    "\tDCE_net.train()\n",
    "\n",
    "\tfor epoch in range(config.num_epochs):\n",
    "\t\tfor iteration, img_lowlight in enumerate(train_loader):\n",
    "\n",
    "\t\t\timg_lowlight = img_lowlight.cuda()\n",
    "\n",
    "\t\t\tenhanced_image_1,enhanced_image,A  = DCE_net(img_lowlight)\n",
    "\n",
    "\t\t\tLoss_TV = 1600*l_tv(A)\n",
    "\t\t\tloss_spa = 0.1 * torch.mean(l_spa(enhanced_image, img_lowlight))\n",
    "\t\t\tloss_col = 5*torch.mean(l_color(enhanced_image))\n",
    "\t\t\tloss_exp = 8*torch.mean(l_exp(enhanced_image))\t\n",
    "\t\t\t\n",
    "\t\t\t# best_loss\n",
    "\t\t\tloss =  Loss_TV + loss_spa + loss_col + loss_exp\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm(DCE_net.parameters(),config.grad_clip_norm)\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\tif ((iteration+1) % config.display_iter) == 0:\n",
    "\t\t\t\tprint(f\"Epoch [{epoch + 1}/{config.num_epochs}] Iter {iteration+1} Loss: {loss.item():.4f}\")\n",
    "    \n",
    "\t\t\t\tglobal_step = epoch * len(train_loader) + iteration\n",
    "                \n",
    "\t\t\t\twriter.add_scalar('Loss/Total', loss.item(), global_step)\n",
    "\t\t\t\twriter.add_scalar('Details/Spatial', loss_spa.item(), global_step)\n",
    "\t\t\t\twriter.add_scalar('Details/Color', loss_col.item(), global_step)\n",
    "\t\t\t\twriter.add_scalar('Details/Exposure', loss_exp.item(), global_step)\n",
    "\t\t\t\twriter.add_scalar('Details/TV_Smoothness', Loss_TV.item(), global_step)\n",
    "\t\t\tif ((iteration+1) % config.snapshot_iter) == 0:\n",
    "\t\t\t\ttorch.save(DCE_net.state_dict(), config.snapshots_folder + \"Epoch\" + str(epoch) + '.pth') \t\t\n",
    "    \n",
    "\twriter.close()\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40d01a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8de6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
